title: SVD
author: gsal
tags:
  - 线性代数
  - 机器学习
categories:
  - 数学
  - 线性代数
date: 2018-05-27 13:04:00
---
## 介绍
SVD指的是Singular Value Decomposition，奇异值分解。简单理解就是把一个矩阵（m × n）的矩阵分解为3部分，即$u∑v^t$。SVD是PCA即主成分分析的一种，就是保留矩阵中比较重要的部分信息，去掉不那么重要的信息。这里面的u和t是正交矩阵，v是对角线矩阵（diagonal matrix），并且每一个元素非负。https://www.youtube.com/watch?v=mBcLRGuAFUk  这个视频在简短的时间对svd作了介绍。

## 如何进行SVD呢？
https://www.youtube.com/watch?v=cOUTpqlX-Xs 这个视频很好地演示了如何对一个简单的矩阵手动进行svd。主要的思路就是在等式两边同时乘以v的转置，就把问题转化为了就特征向量和特征值的问题了。目前我还没有搞清楚的点是，正交矩阵有些什么特点，为什么  
$$A\cdot A^t = I$$ 
以及求出某个矩阵的特征值之后怎样求出它的特征向量？除此之外SVD是怎样应用到机器学习上的，也值得进一步的了解。

## 正交矩阵
首先，什么是正交矩阵。正交矩阵一定是一个方阵。它的每一列都是一个向量，这个向量和其他的这个矩阵里的列向量是正交的。（点乘起来是0，自己和自己点乘是1（这个列向量单位化为1））。正交矩阵的性质：
$$Q\cdot Q^t = I $$
也就是$$ Q^{-1} = Q^t $$